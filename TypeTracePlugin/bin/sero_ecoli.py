#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Author: Ming
# @Date:   2022-05-29 21:12:37
# @Last Modified by:   Ming
# @Last Modified time: 2022-05-29 21:33:37
from __future__ import division
from argparse import ArgumentParser
from tabulate import tabulate
from cgecore.blaster import Blaster
from cgecore.cgefinder import CGEFinder
from distutils.spawn import find_executable
import sys, os, time, re, subprocess
import json, gzip, pprint


# 大肠埃希菌血清型分析
##########################################################################
# FUNCTIONS
##########################################################################
def text_table(headers, rows, empty_replace='-'):
    ''' Create text table

    USAGE:
       >>> from tabulate import tabulate
       >>> headers = ['A','B']
       >>> rows = [[1,2],[3,4]]
       >>> print(text_table(headers, rows))
       **********
         A    B
       **********
         1    2
         3    4
       ==========
    '''
    # Replace empty cells with placeholder
    rows = map(lambda row: map(lambda x: x if x else empty_replace, row), rows)
    # Create table
    table = tabulate(rows, headers, tablefmt='simple').split('\n')
    # Prepare title injection
    width = len(table[0])
    # Switch horisontal line
    table[1] = '*' * (width + 2)
    # Update table with title
    table = ("%s\n" * 3) % ('*' * (width + 2), '\n'.join(table), '=' * (width + 2))
    return table


def is_gzipped(file_path):
    ''' Returns True if file is gzipped and False otherwise.
        The result is inferred from the first two bits in the file read
        from the input path.
        On unix systems this should be: 1f 8b
        Theoretically there could be exceptions to this test but it is
        unlikely and impossible if the input files are otherwise expected
        to be encoded in utf-8.
    '''
    with open(file_path, mode='rb') as fh:
        bit_start = fh.read(2)
    if (bit_start == b'\x1f\x8b'):
        return True
    else:
        return False


def get_file_format(input_files):
    """
    Takes all input files and checks their first character to assess
    the file format. Returns one of the following strings; fasta, fastq,
    other or mixed. fasta and fastq indicates that all input files are
    of the same format, either fasta or fastq. other indiates that all
    files are not fasta nor fastq files. mixed indicates that the inputfiles
    are a mix of different file formats.
    """

    # Open all input files and get the first character
    file_format = []
    invalid_files = []
    for infile in input_files:
        if is_gzipped(infile):  # [-3:] == ".gz":
            f = gzip.open(infile, "rb")
            fst_char = f.read(1);
        else:
            f = open(infile, "rb")
            fst_char = f.read(1);
        f.close()
        # Assess the first character
        if fst_char == b"@":
            file_format.append("fastq")
        elif fst_char == b">":
            file_format.append("fasta")
        else:
            invalid_files.append("other")
    if len(set(file_format)) != 1:
        return "mixed"
    return ",".join(set(file_format))


def make_aln(file_handle, json_data, query_aligns, homol_aligns, sbjct_aligns):
    for db_name, db_info in json_data.items():
        if isinstance(db_info, str):
            continue
        else:
            for gene_id, gene_info in sorted(db_info.items(), key=lambda x: (x[1]['gene'], x[1]['accession'])):
                seq_name = gene_info["gene"] + "_" + gene_info["accession"]
                hit_name = gene_info["hit_id"]

                seqs = ["", "", ""]
                seqs[0] = sbjct_aligns[db_name][hit_name]
                seqs[1] = homol_aligns[db_name][hit_name]
                seqs[2] = query_aligns[db_name][hit_name]

                write_align(seqs, seq_name, file_handle)


def write_align(seq, seq_name, file_handle):
    file_handle.write("# {}".format(seq_name) + "\n")
    sbjct_seq = seq[0]
    homol_seq = seq[1]
    query_seq = seq[2]
    for i in range(0, len(sbjct_seq), 60):
        file_handle.write("%-10s\t%s\n" % ("template:", sbjct_seq[i:i + 60]))
        file_handle.write("%-10s\t%s\n" % ("", homol_seq[i:i + 60]))
        file_handle.write("%-10s\t%s\n\n" % ("query:", query_seq[i:i + 60]))


##########################################################################
# PARSE COMMAND LINE OPTIONS
##########################################################################

parser = ArgumentParser()
parser.add_argument("-i", "--infile", dest="infile", help="FASTA or FASTQ input files.", nargs="+", required=True)
parser.add_argument("-o", "--outputPath", dest="outdir", help="Path to blast output", default='.')
parser.add_argument("-tmp", "--tmp_dir",
                    help="Temporary directory for storage of the results from the external software.")
parser.add_argument("-mp", "--methodPath", dest="method_path", help="Path to method to use (kma or blastn)")
parser.add_argument("-p", "--databasePath", dest="db_path", help="Path to the databases", default='/database')
parser.add_argument("-d", "--databases", dest="databases",
                    help="Databases chosen to search in - if non is specified all is used")
parser.add_argument("-l", "--mincov", dest="min_cov", help="Minimum coverage", default=0.60)
parser.add_argument("-t", "--threshold", dest="threshold", help="Minimum threshold for identity", default=0.90)
parser.add_argument("-x", "--extented_output",
                    help="Give extented output with allignment files, template and query hits in fasta and\
                          a tab seperated file with gene profile results", action="store_true")
parser.add_argument("-q", "--quiet", action="store_true")

args = parser.parse_args()

##########################################################################
# MAIN
##########################################################################

if args.quiet:
    f = open('/dev/null', 'w')
    sys.stdout = f

# Defining varibales
min_cov = float(args.min_cov)
threshold = float(args.threshold)
method_path = args.method_path

# Check if valid database is provided
if args.db_path is None:
    sys.exit("Input Error: No database directory was provided!\n")
elif not os.path.exists(args.db_path):
    sys.exit("Input Error: The specified database directory does not"
             " exist!\n")
else:
    # Check existence of config file
    db_config_file = '%s/config' % (args.db_path)
    if not os.path.exists(db_config_file):
        sys.exit("Input Error: The database config file could not be "
                 "found!")
    # Save path
    db_path = args.db_path

# Check if valid input files are provided
if args.infile is None:
    sys.exit("Input Error: No input file was provided!\n")
elif not os.path.exists(args.infile[0]):
    sys.exit("Input Error: Input file does not exist!\n")
elif len(args.infile) > 1:
    if not os.path.exists(args.infile[1]):
        sys.exit("Input Error: Input file does not exist!\n")

infile = args.infile

# Check if valid output directory is provided
if not os.path.exists(args.outdir):
    sys.exit("Input Error: Output dirctory does not exist!\n")
outdir = os.path.abspath(args.outdir)

# Check if valid tmp directory is provided
if args.tmp_dir:
    if not os.path.exists(args.tmp_dir):
        sys.exit("Input Error: Tmp dirctory, {}, does not exist!\n".format(args.tmp_dir))
    else:
        tmp_dir = os.path.abspath(args.tmp_dir)
else:
    tmp_dir = outdir

# Check if databases and config file are correct/correponds
dbs = dict()
extensions = []
db_description = {}
with open(db_config_file) as f:
    for l in f:
        l = l.strip()
        if l == '': continue
        if l[0] == '#':
            if 'extensions:' in l:
                extensions = [s.strip() for s in l.split('extensions:')[-1].split(',')]
            continue
        tmp = l.split('\t')
        if len(tmp) != 3:
            sys.exit(("Input Error: Invalid line in the database"
                      " config file!\nA proper entry requires 3 tab "
                      "separated columns!\n%s") % (l))
        db_prefix = tmp[0].strip()
        name = tmp[1].split('#')[0].strip()
        db_description[name] = tmp[2]

        # Check if all db files are present
        for ext in extensions:
            db = "%s/%s.%s" % (db_path, db_prefix, ext)
            if not os.path.exists(db):
                sys.exit(("Input Error: The database file (%s) "
                          "could not be found!") % (db))
        if db_prefix not in dbs: dbs[db_prefix] = []
        dbs[db_prefix].append(name)
if len(dbs) == 0:
    sys.exit("Input Error: No databases were found in the "
             "database config file!")

if args.databases is None:
    # Choose all available databases from the config file
    databases = dbs.keys()
else:
    # Handle multiple databases
    args.databases = args.databases.split(',')
    # Check that the ResFinder DBs are valid
    databases = []
    for db_prefix in args.databases:
        if db_prefix in dbs:
            databases.append(db_prefix)
        else:
            sys.exit("Input Error: Provided database was not "
                     "recognised! (%s)\n" % db_prefix)

species = [",".join(dbs[db]) for db in databases]

# Check file format (fasta, fastq or other format)
file_format = get_file_format(infile)

# Call appropriate method (kma or blastn) based on file format
if file_format == "fastq":
    if not method_path:
        method_path = "kma"
    if find_executable(method_path) == None:
        sys.exit("No valid path to a kma program was provided. Use the -mp flag to provide the path.")
    # Check the number of files
    if len(infile) == 1:
        infile_1 = infile[0]
        infile_2 = None
    elif len(infile) == 2:
        infile_1 = infile[0]
        infile_2 = infile[1]
    else:
        sys.exit("Only 2 input file accepted for raw read data,\
                if data from more runs is avaliable for the same\
                sample, please concatinate the reads into two files")

    sample_name = os.path.basename(sorted(args.infile)[0])
    method = "kma"

    # Call KMA
    method_obj = CGEFinder.kma(infile_1, tmp_dir, databases, db_path, min_cov=min_cov,
                               threshold=threshold, kma_path=method_path, sample_name=sample_name,
                               inputfile_2=infile_2, kma_mrs=0.75, kma_gapopen=-5,
                               kma_gapextend=-1, kma_penalty=-3, kma_reward=1)
elif file_format == "fasta":
    if not method_path:
        method_path = "blastn"
    if find_executable(method_path) == None:
        sys.exit("No valid path to a blastn program was provided. Use the -mp flag to provide the path.")
    # Assert that only one fasta file is inputted
    assert len(infile) == 1, "Only one input file accepted for assembled data"
    infile = infile[0]
    method = "blast"

    # Call BLASTn
    method_obj = Blaster(infile, databases, db_path, tmp_dir,
                         min_cov, threshold, method_path, cut_off=False)
else:
    sys.exit("Input file must be fastq or fasta format, not " + file_format)

results = method_obj.results
query_aligns = method_obj.gene_align_query
homo_aligns = method_obj.gene_align_homo
sbjct_aligns = method_obj.gene_align_sbjct

json_results = dict()

hits = []

for db in results:
    contig_res = {}
    if db == 'excluded':
        continue
    db_name = str(dbs[db][0])
    if db_name not in json_results:
        json_results[db_name] = {}
    if results[db_name] == "No hit found":
        json_results[db_name] = "No hit found"
    else:
        for contig_id, hit in results[db].items():

            identity = float(hit["perc_ident"])
            coverage = float(hit["perc_coverage"])

            # Skip hits below coverage
            if coverage < (min_cov * 100) or identity < (threshold * 100):
                continue

            bit_score = identity * coverage

            if contig_id not in contig_res:
                contig_res[contig_id] = []
            contig_res[contig_id].append([hit["query_start"], hit["query_end"], bit_score, hit])

    # Check for overlapping hits, only report the best
    for contig_id, hit_lsts in contig_res.items():

        hit_lsts.sort(key=lambda x: x[0])
        hits = [hit[3] for hit in hit_lsts]

        # Get information from the fisrt hit found
        current_end = hit_lsts[0][1]
        current_bit_score = hit_lsts[0][2]

        # Check if more then one hit was found within the same gene
        for i in range(len(hit_lsts) - 1):

            # Save information from next hit
            next_start = hit_lsts[i + 1][0]
            next_end = hit_lsts[i + 1][1]
            next_bit_score = hit_lsts[i + 1][2]

            # Check for overlapping sequences
            # <--------------->
            #            <------------>
            if next_start < current_end:
                # Delete the hit with lowest bit score from the hit list
                # <----->
                #   <-------------------->
                if current_bit_score < next_bit_score:
                    del hits[i]
                    # reset current end and score
                    current_end = next_end
                    current_bit_score = next_bit_score
                # <-------------------->
                #                  <----->
                else:
                    # Delete next hit, keep current end and bit score
                    del hits[i + 1]

        for hit in hits:
            header = hit["sbjct_header"]
            tmp = header.split("_")
            gene = tmp[0]
            acc = tmp[2]
            serotype = tmp[3]
            identity = hit["perc_ident"]
            coverage = hit["perc_coverage"]
            sbj_length = hit["sbjct_length"]
            HSP = hit["HSP_length"]
            positions_contig = "%s..%s" % (hit["query_start"], hit["query_end"])
            positions_ref = "%s..%s" % (hit["sbjct_start"], hit["sbjct_end"])
            contig_name = hit["contig_name"]

            # Write JSON results dict
            json_results[db_name].update({header: {}})
            json_results[db_name][header] = {"gene": gene, "serotype": serotype, "identity": round(identity, 2),
                                             "HSP_length": HSP, "template_length": sbj_length,
                                             "position_in_ref": positions_ref,
                                             "contig_name": contig_name, "positions_in_contig": positions_contig,
                                             "accession": acc, "coverage": round(coverage, 2), "hit_id": contig_id}

# Get run info for JSON file
service = os.path.basename(__file__).replace(".py", "")
date = time.strftime("%d.%m.%Y")
time = time.strftime("%H:%M:%S")

# Make JSON output file
data = {service: {}}

userinput = {"filename(s)": args.infile, "method": method, "file_format": file_format}
run_info = {"date": date, "time": time}

data[service]["user_input"] = userinput
data[service]["run_info"] = run_info
data[service]["results"] = json_results

pprint.pprint(data)

# Save json output
result_file = "{}/data.json".format(outdir)
with open(result_file, "w") as outfile:
    json.dump(data, outfile)

# Getting and writing out the results
header = ["Gene", "Serotype", "Identity", "Template / HSP length", "Contig", "Position in contig", "Accession number"]

if args.extented_output:
    # Define extented output
    table_filename = "{}/results_tab.tsv".format(outdir)
    query_filename = "{}/Hit_in_genome_seq.fsa".format(outdir)
    sbjct_filename = "{}/Serotype_allele_seq.fsa".format(outdir)
    result_filename = "{}/results.txt".format(outdir)
    table_file = open(table_filename, "w")
    query_file = open(query_filename, "w")
    sbjct_file = open(sbjct_filename, "w")
    result_file = open(result_filename, "w")

    # Make results file
    result_file.write("{} Results\n\nDatabase(s): {}\n\n".format(service, ",".join(set(species))))

    # Write tsv table
    rows = [["Database"] + header]

    for db_name, db_hits in json_results.items():
        result_file.write("*" * len("\t".join(header)) + "\n")
        result_file.write(db_description[db_name] + "\n")
        db_rows = []

        # Check it hits are found
        if isinstance(db_hits, str):
            content = [''] * len(header)
            content[int(len(header) / 2)] = db_hits
            result_file.write(text_table(header, [content]) + "\n")
            # result_file.write("*"*len("\t".join(header)) + "\n")
            # result_file.write("\t".join(header) + "\n")
            # result_file.write("*"*len("\t".join(header)) + "\n")
            # result_file.write(db_hits.rstrip() + "\n")
            # result_file.write("="*len("\t".join(header)) + "\n")
            continue

        for gene_id, gene_info in sorted(db_hits.items(), key=lambda x: (x[1]['serotype'], x[1]['accession'])):
            vir_gene = gene_info["gene"]
            serotype = gene_info["serotype"]
            identity = str(gene_info["identity"])
            coverage = str(gene_info["coverage"])
            template_HSP = str(gene_info["HSP_length"]) + " / " + str(gene_info["template_length"])
            position_in_ref = gene_info["position_in_ref"]
            position_in_contig = gene_info["positions_in_contig"]
            acc = gene_info["accession"]
            contig_name = gene_info["contig_name"]

            # Add rows to result tables
            db_rows.append([vir_gene, serotype, identity, template_HSP, contig_name, position_in_contig, acc])
            rows.append([db_name, vir_gene, serotype, identity, template_HSP, contig_name, position_in_contig, acc])
            # Write query fasta output
            hit_name = gene_info["hit_id"]
            query_seq = query_aligns[db_name][hit_name]
            sbjct_seq = sbjct_aligns[db_name][hit_name]

            if coverage == "100.0" and identity == "100.0":
                match = "PERFECT MATCH"
            else:
                match = "WARNING"
            qry_header = ">{}:{} ID:{}% COV:{}% Best_match:{}\n".format(vir_gene, match, identity,
                                                                        coverage, gene_id)
            query_file.write(qry_header)
            for i in range(0, len(query_seq), 60):
                query_file.write(query_seq[i:i + 60] + "\n")

            # Write template fasta output
            sbj_header = ">{}\n".format(gene_id)
            sbjct_file.write(sbj_header)
            for i in range(0, len(sbjct_seq), 60):
                sbjct_file.write(sbjct_seq[i:i + 60] + "\n")

        # Write db results tables in results file and table file
        # db_rows.sort(key=lambda x: x[0])
        result_file.write(text_table(header, db_rows) + "\n")

    for row in rows:
        table_file.write("\t".join(row) + "\n")

    # Write allignment output
    result_file.write("\n\nExtended Output:\n\n")
    make_aln(result_file, json_results, query_aligns, homo_aligns, sbjct_aligns)

    # Close all files
    query_file.close()
    sbjct_file.close()
    table_file.close()
    result_file.close()

if args.quiet:
    f.close()
